{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a60324-f7fc-4e67-b82f-d3ad97b1f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias de PySpark\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.functions import col, when, lit, count, sum, max, date_sub\n",
    "\n",
    "# En Databricks, la sesión 'spark' ya está creada.\n",
    "# Las rutas de los archivos deben apuntar a su ubicación en DBFS (Databricks File System).\n",
    "# Por ejemplo: \"/mnt/data/mercado_pago/prints.json\"\n",
    "prints_path = \"prints.json\"\n",
    "taps_path = \"taps.json\"\n",
    "pays_path = \"pays.csv\"\n",
    "output_path = \"output_feature_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969377d8-c633-4197-af48-c89e87014446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Cargar y aplanar Prints\n",
    "prints_df = spark.read.json(prints_path) \\\n",
    "    .select(\n",
    "        col(\"day\").cast(\"date\"),\n",
    "        col(\"user_id\").cast(\"long\"),\n",
    "        col(\"event_data.position\").cast(\"integer\").alias(\"position\"),\n",
    "        col(\"event_data.value_prop\").alias(\"value_prop\")\n",
    "    )\n",
    "\n",
    "# 2.2 Cargar y aplanar Taps\n",
    "taps_df = spark.read.json(taps_path) \\\n",
    "    .select(\n",
    "        col(\"day\").cast(\"date\"),\n",
    "        col(\"user_id\").cast(\"long\"),\n",
    "        col(\"event_data.position\").cast(\"integer\").alias(\"position\"),\n",
    "        col(\"event_data.value_prop\").alias(\"value_prop\")\n",
    "    )\n",
    "\n",
    "# 2.3 Cargar Payments\n",
    "pays_df = spark.read.csv(pays_path, header=True, inferSchema=True) \\\n",
    "    .select(\n",
    "        col(\"pay_date\").cast(\"date\"),\n",
    "        col(\"total\").cast(\"double\"),\n",
    "        col(\"user_id\").cast(\"long\"),\n",
    "        col(\"value_prop\")\n",
    "    )\n",
    "\n",
    "print(\"Datos cargados y preparados. Mostrando una muestra de 'prints':\")\n",
    "prints_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235678a-ad4f-4f25-81a2-20f7736530ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Definir el periodo de análisis\n",
    "max_date = prints_df.agg(max(\"day\")).first()[0]\n",
    "last_week_start_date = max_date - timedelta(days=6)\n",
    "\n",
    "print(f\"Fecha máxima en los datos: {max_date}\")\n",
    "print(f\"Inicio de la última semana (periodo base): {last_week_start_date}\")\n",
    "\n",
    "# 3.2 Crear el DataFrame base: prints de la última semana\n",
    "base_df = prints_df.filter(col(\"day\") >= last_week_start_date)\n",
    "\n",
    "print(f\"\\nNúmero de prints en la última semana: {base_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051981da-fcf7-49f9-89fb-b6747b3a8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un LEFT JOIN con los taps para identificar los clics\n",
    "taps_with_flag = taps_df.withColumn(\"clicked\", lit(1))\n",
    "join_cols = [\"day\", \"user_id\", \"position\", \"value_prop\"]\n",
    "\n",
    "base_df = base_df.join(taps_with_flag, join_cols, \"left\") \\\n",
    "    .withColumn(\"was_clicked\", when(col(\"clicked\").isNotNull(), 1).otherwise(0)) \\\n",
    "    .drop(\"clicked\")\n",
    "\n",
    "print(\"Feature 'was_clicked' añadida. Muestra:\")\n",
    "base_df.select(\"user_id\", \"value_prop\", \"was_clicked\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05fea8-5f54-4e03-8d9e-19c35c13e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los datos al periodo histórico (todo antes de la última semana)\n",
    "historical_prints = prints_df.filter(col(\"day\") < last_week_start_date)\n",
    "historical_taps = taps_df.filter(col(\"day\") < last_week_start_date)\n",
    "historical_pays = pays_df.filter(col(\"pay_date\") < last_week_start_date)\n",
    "\n",
    "# Calculamos las agregaciones\n",
    "user_prints_history = historical_prints.groupBy(\"user_id\", \"value_prop\").agg(count(\"*\").alias(\"prints_3w_before\"))\n",
    "user_taps_history = historical_taps.groupBy(\"user_id\", \"value_prop\").agg(count(\"*\").alias(\"taps_3w_before\"))\n",
    "user_pays_history = historical_pays.groupBy(\"user_id\", \"value_prop\").agg(\n",
    "    count(\"*\").alias(\"pays_3w_before\"),\n",
    "    sum(\"total\").alias(\"amount_3w_before\")\n",
    ")\n",
    "\n",
    "print(\"Agregaciones históricas calculadas. Muestra de historial de pagos:\")\n",
    "user_pays_history.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314f601-ef95-4ff4-92c7-abc9cbc4f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos todas las features históricas al DataFrame base\n",
    "final_df = base_df.join(user_prints_history, [\"user_id\", \"value_prop\"], \"left\") \\\n",
    "                  .join(user_taps_history, [\"user_id\", \"value_prop\"], \"left\") \\\n",
    "                  .join(user_pays_history, [\"user_id\", \"value_prop\"], \"left\")\n",
    "\n",
    "# Rellenamos los nulos que significan \"sin actividad\" con 0\n",
    "final_df = final_df.fillna(0, subset=[\n",
    "    \"prints_3w_before\", \"taps_3w_before\", \"pays_3w_before\", \"amount_3w_before\"\n",
    "])\n",
    "\n",
    "# Seleccionamos y ordenamos las columnas para el resultado final\n",
    "final_df = final_df.select(\n",
    "    \"day\", \"user_id\", \"position\", \"value_prop\", \"was_clicked\",\n",
    "    \"prints_3w_before\", \"taps_3w_before\", \"pays_3w_before\", \"amount_3w_before\"\n",
    ").orderBy(\"day\", \"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d9aea-9caf-49fc-8681-08a75f325e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos una muestra del dataset final\n",
    "print(\"Dataset de features finalizado. Muestra:\")\n",
    "display(final_df) # 'display' en Databricks ofrece una vista enriquecida\n",
    "\n",
    "# Guardamos el resultado en formato Delta Lake\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "print(f\"\\n¡Proceso completado! El dataset ha sido guardado en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
